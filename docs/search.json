[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentos del aprendizaje autom√°tico en python",
    "section": "",
    "text": "Bienvenida",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#descripci√≥n-del-programa",
    "href": "index.html#descripci√≥n-del-programa",
    "title": "Fundamentos del aprendizaje autom√°tico en python",
    "section": "Descripci√≥n del programa",
    "text": "Descripci√≥n del programa\nAprende el arte del aprendizaje autom√°tico y sal como un Jefe de predicci√≥n, reconocimiento de patrones y los inicios del Aprendizaje Profundo y de Refuerzo.",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "01_Aprendizaje_supervisado_con_scikitlearn/index.html",
    "href": "01_Aprendizaje_supervisado_con_scikitlearn/index.html",
    "title": "Bienvenida",
    "section": "",
    "text": "Descripci√≥n\nüìä Nivel: Intermedio\nüïí Duraci√≥n estimada: 4 horas\nüé• Incluye c√≥digo, visualizaciones y ejercicios\nDesarrolla tus habilidades en machine learning con scikit-learn y descubre c√≥mo utilizar esta popular biblioteca de Python para entrenar modelos utilizando datos etiquetados. En este curso, aprender√°s a hacer predicciones potentes, como si un cliente se dar√° de baja de tu negocio, si una persona tiene diabetes e incluso c√≥mo clasificar el g√©nero de una canci√≥n. Utilizando conjunto de datos del mundo real, descubrir√°s c√≥mo construir modelos predictivos, ajustar sus par√°metros y determinar su rendimiento con datos no vistos.",
    "crumbs": [
      "Aprendizaje supervisado con scikit-learn",
      "Bienvenida"
    ]
  },
  {
    "objectID": "01_Aprendizaje_supervisado_con_scikitlearn/index.html#m√≥dulos-del-curso",
    "href": "01_Aprendizaje_supervisado_con_scikitlearn/index.html#m√≥dulos-del-curso",
    "title": "Bienvenida",
    "section": "M√≥dulos del curso",
    "text": "M√≥dulos del curso\n\nClasificaci√≥n\nRegresi√≥n\nAfinar tu Modelo\nPreprocesamiento y Canalizaciones",
    "crumbs": [
      "Aprendizaje supervisado con scikit-learn",
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#cursos-del-programa",
    "href": "index.html#cursos-del-programa",
    "title": "Fundamentos del aprendizaje autom√°tico en python",
    "section": "Cursos del Programa",
    "text": "Cursos del Programa\n\nAprendizaje supervisado con scikit-learn\nProyecto - Predictive Modeling for Agriculture\nAprendizaje no supervisado en python\nProyecto - Clustering Antartic Pinguin Species\nIntroducci√≥n al aprendizaje profundo con PyTorch\nReinforcement Learning with Gymnasium in Python\nProyecto - Taxi Route Optimization with Reinforcement Learning",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html",
    "href": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html",
    "title": "Clasificaci√≥n",
    "section": "",
    "text": "Machine learning con scikit-learn\nEn este cap√≠tulo, se te presentar√°n los problemas de clasifiaci√≥n y aprender√°s a resolverlos mediante t√©cnicas de aprendizaje supervisado. Aprender√°s a dividir los datos en conjuntos de entrenamiento y de prueba, ajustar un modelo, hacer predicciones y evaluar la precisi√≥n. Descubrir√°s la relaci√≥n entre la complejidad del modelo y el rendimiento, aplicando lo que aprendas a un conjunto de datos de rotaci√≥n, donde clasificar√°s el estado de rotaci√≥n de los clientes de una empresa de telecomunicaciones.\nfrom sklearn.module import Model\nmodel = Model()\nmodel.fit(X, y)\npredictions = model.predict(X_new)\nprint(predictions)",
    "crumbs": [
      "Aprendizaje supervisado con scikit-learn",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html#machine-learning-con-scikit-learn",
    "href": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html#machine-learning-con-scikit-learn",
    "title": "Clasificaci√≥n",
    "section": "",
    "text": "¬øQu√© es el machine learning?\n\nEs el proceso mediante el cual:\n\nSe da a los ordenadores la capacidad de aprender a tomar decisiones a partir de datos.\n¬°Sin haber sido programados expl√≠citamente!\n\n\nEjemplos de machine learning\n\nClasificaci√≥n de correos de spam o no en relaci√≥n al contenido y su remitente.\nAgrupar libros por categor√≠as en funci√≥n de las palabras que contiene.\n\nAprendizaje no supervisado\n\nDescubrir patrones ocultos a partir de datos no etiquetados.\nEjemplo\n\nAgrupar a los clientes en categor√≠as distintas (agrupaci√≥n en cl√∫steres)\n\n\n\nAprendizaje supervisado\n\nLos valores previstos son conocidos.\nObjetivo: predecir los valores objetivo de los datos no vistos, dadas las caracter√≠sticas.\n\n\nTipos de aprendizaje supervisado\n\n\n\n\n\n\n\n\nClasificaci√≥n: La variable objetivo consta de categor√≠as.\nEjemplo: Predecir si una transferencia bancaria es fraudulenta o no.\n\n\nRegresi√≥n: La variable objetivo es continua.\nEjemplo: Un modelo puede utilizar caracter√≠sticas como el n√∫mero de habitaciones y el tama√±o de una propiedad para predecir el precio de la propiedad.\n\n\n\n\nConvenciones de denominaci√≥n\n\nCaracter√≠stica = variable predictora = variable independiente\nVariable objetivo = variable dependiente = variable de respuesta\n\nAntes de utilizar el aprendizaje supervisado\n\nRequisitos:\n\nQue no falten valores.\nQue los datos esten en formato num√©rico.\nQue los datos esten almacenados en un DataFrame de pandas o en una matriz de Numpy.\n\nHay que realizar primero un an√°lisis exploratorio de datos (AED)\n\nSintaxis de scikit-learn\n\nScikit-learn sigue la misma sintaxis para todos los modelos de aprendizaje supervisado lo que hace que el flujo de trabajo sea reproducible.\n\n\n\n\n\nClasificaci√≥n binaria\nHay dos tipos de aprendizaje supervisado: clasificaci√≥n y regresi√≥n. La clasificaci√≥n binaria se utiliza para predecir una variable objetivo que solo tiene dos etiquetas, normalmente representados num√©ricamente con un cero o un uno.\nA continuaci√≥n se muestra la p√°gina .head() de un conjunto de datos churn_df. Puedes esperar que el resto de los datos contengan valores similares.\n\nObservando estos datos, ¬øQu√© columna podr√≠a ser la variable objetivo de la clasificaci√≥n binaria?\nRespuestas posibles\n\n‚Äúcustomer_service_calls‚Äù\n‚Äútotal_night_charge‚Äù\n‚Äúchurn‚Äù\n‚Äúaccount_lenght‚Äù\n\nchurn tiene valores de 0 y 1, por lo que se puede predecir utilizando un modelo de clasificaci√≥n binaria.\n\n\nEl flujo de trabajo del aprendizaje supervisado\nRecuerda que sckikit-learn ofrece un flujo de trabajo repetible para utilizar modelos de aprendizaje supervisado con el fin de predecir los valores de la variable objetivo cuando se presentan nuevos datos.\nReordena el pseudoc√≥digo proporcionado para que represente con precisi√≥n el flujo de trabajo de construir un modelo de aprendizaje supervisado y hacer predicciones.\n\nInstrucciones\n\nArrastra los bloques de c√≥digo en el orden correcto para representar c√≥mo se ejecutar√≠a el flujo de trabajo de aprendizaje supervisado.\n\n\nPuedes ver c√≥mo scikit-learn permite hacer predicciones ¬°con solo unas pocas l√≠neas de c√≥digo!",
    "crumbs": [
      "Aprendizaje supervisado con scikit-learn",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html#el-reto-de-la-clasificaci√≥n",
    "href": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html#el-reto-de-la-clasificaci√≥n",
    "title": "Clasificaci√≥n",
    "section": "El reto de la clasificaci√≥n",
    "text": "El reto de la clasificaci√≥n\n\nClasificar etiquetas de datos no vistos\n\n\nSe construye un modelo.\nEl modelo aprende de los datos etiquetados que le pasamos.\nLe pasamos datos no etiquetados al modelo como entrada.\nEl modelo predice las etiquetas de los datos no vistos.\n\nDatos etiquetados = datos de entrenamiento\n\n\n\nK vecinos m√°s cercanos (KNN)\n\nPredecir la etiqueta de un punto de datos:\n\nObservando los k puntos de datos etiquetados m√°s cercanos.\nUtilizando el voto por mayor√≠a.\n\n\n\n\n\n\nsi k = 3\nsi k = 5\n\n\n\n\n\n\nSe clasificar√≠a como roja\nSe clasificar√≠a como azul\n\n\n\n\nIntuici√≥n KNN\nSe muestra un gr√°fico de dispersi√≥n que muestra la tarifa nocturna total frente a la tarifa diaria total de los clientes de una empresa de telecomunicaciones. Azul representa a los clientes que se han dado de baja y los de rojo a los que no.\n\n\n\n\n\n\n\n\nKNN crea un l√≠mite de decisi√≥n para predecir si los clientes se dar√°n de baja. Se prevee que los clientes en la franja con fondo gris, se dar√°n de baja.\nUtilizar scikit-learn para ajustar un clasificador\n\nfrom sklearn.neighbors import KNeighborsClassifier\nX = churn_df[[\"total_day_charge\", \"total_eve_charge\"]].values\ny = churn_df[\"churn\"].values\nprint(X.shape, y.shape)\n\n\n\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X, y)\n\nPredecir con datos no etiquetados\n\n\nX_new = np.array([[56.8, 17.5],\n                  [24.4, 24.1],\n                  [50.1, 10.9]])\n\nprint(X_new.shape)\n\n\n\npredictions = knn.predict(X_new)\nprint('Predictions: {}' .format(predictions))\n\n\n\nAjustar KNN: k vecinos m√°s cercanos\nEn este ejercicio, construir√°s tu primer modelo de clasificaci√≥n utilizando el conjunto de datos churn_df, que se ha precargado para el resto del cap√≠tulo.\nEl objetivo, churn tiene que ser una sola columna con el mismo m√∫mero de observaciones que los datos de las caracter√≠sticas. Los datos de las caracter√≠sticas ya se han convertido en matrices numpy.\n‚Äúaccount_length‚Äù y ‚Äúcostumer_service_calls‚Äù se tratan como caracter√≠sticas porque la duraci√≥n de la cuenta indica fidelidad del cliente, las llamadas frecuentes al servicio de atenci√≥n al cliente pueden ser se√±al de insatisfacci√≥n y ambas pueden ser buenos predictores de la rotaci√≥n.\n\nInstrucciones\n\nImporta KNeighborsClassifier desde sklearn.neighbors.\nInstncia un KNeighborsClassifier llamado knn con 6 vecinos.\nAjusta el clasificador a los datos utilizando el m√©todo .fit().\n\n\nimport pandas as pd\n\nruta = './data/churn_df.csv'\nchurn_df = pd.read_csv(ruta)\nchurn_df.head()\n\n\n\n\n\n\n\n\naccount_length\ntotal_day_charge\ntotal_eve_charge\ntotal_night_charge\ntotal_intl_charge\ncustomer_service_calls\nchurn\n\n\n\n\n0\n101\n45.85\n17.65\n9.64\n1.22\n3\n1\n\n\n1\n73\n22.30\n9.05\n9.98\n2.75\n2\n0\n\n\n2\n86\n24.62\n17.53\n11.49\n3.13\n4\n0\n\n\n3\n59\n34.73\n21.02\n9.66\n3.24\n1\n0\n\n\n4\n129\n27.42\n18.75\n10.11\n2.59\n1\n0\n\n\n\n\n\n\n\n\n# Import KNeigborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny = churn_df['churn'].values\nX = churn_df[['account_length', 'customer_service_calls']].values\n\n# Create a KNN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X, y)\n\nKNeighborsClassifier(n_neighbors=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nn_neighbors n_neighbors: int, default=5\n\nNumber of neighbors to use by default for :meth:`kneighbors` queries.\n6\n\n\n\nweights weights: {'uniform', 'distance'}, callable or None, default='uniform'\n\nWeight function used in prediction. Possible values:\n\n- 'uniform' : uniform weights. All points in each neighborhood\nare weighted equally.\n- 'distance' : weight points by the inverse of their distance.\nin this case, closer neighbors of a query point will have a\ngreater influence than neighbors which are further away.\n- [callable] : a user-defined function which accepts an\narray of distances, and returns an array of the same shape\ncontaining the weights.\n\nRefer to the example entitled\n:ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\nshowing the impact of the `weights` parameter on the decision\nboundary.\n'uniform'\n\n\n\nalgorithm algorithm: {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n\nAlgorithm used to compute the nearest neighbors:\n\n- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm\nbased on the values passed to :meth:`fit` method.\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force.\n'auto'\n\n\n\nleaf_size leaf_size: int, default=30\n\nLeaf size passed to BallTree or KDTree. This can affect the\nspeed of the construction and query, as well as the memory\nrequired to store the tree. The optimal value depends on the\nnature of the problem.\n30\n\n\n\np p: float, default=2\n\nPower parameter for the Minkowski metric. When p = 1, this is equivalent\nto using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\nFor arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\nto be positive.\n2\n\n\n\nmetric metric: str or callable, default='minkowski'\n\nMetric to use for distance computation. Default is \"minkowski\", which\nresults in the standard Euclidean distance when p = 2. See the\ndocumentation of `scipy.spatial.distance\n&lt; docs.scipy.org doc scipy reference spatial.distance.html&gt;`_ and\nthe metrics listed in\n:class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\nvalues.\n\nIf metric is \"precomputed\", X is assumed to be a distance matrix and\nmust be square during fit. X may be a :term:`sparse graph`, in which\ncase only \"nonzero\" elements may be considered neighbors.\n\nIf metric is a callable function, it takes two arrays representing 1D\nvectors as inputs and must return one value indicating the distance\nbetween those vectors. This works for Scipy's metrics, but is less\nefficient than passing the metric name as a string.\n'minkowski'\n\n\n\nmetric_params metric_params: dict, default=None\n\nAdditional keyword arguments for the metric function.\nNone\n\n\n\nn_jobs n_jobs: int, default=None\n\nThe number of parallel jobs to run for neighbors search.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary `\nfor more details.\nDoesn't affect :meth:`fit` method.\nNone\n\n\n\n\n            \n        \n    \n\n\n¬°Excelente! Ahora que tu clasificador KNN ha sido ajustado a los datos, puede ser utilizado para predecir las etiquetas de nuevos puntos de datos.\n\n\n\nPredecir KNN: k vecinos m√°s cercanos\nAhora que has ajustado tu clasificador KNN, puedes utilizarlo para predecir la etiqueta de nuevos puntos de datos. Para el entrenamiento se utilizaron todos los datos disponibles, pero, afortunadamente, hay nuevas observaciones disponibles. Se han precargado como X_new.\n\nimport numpy as np\nX_new = np.array([[30.0, 17.5],\n                  [107.0, 24.1],\n                  [213.0, 10.9]])\n\n\nInstrucciones\n\nCrea y_pred prediciendo los valores objetivo de las caracter√≠sticas no vistas X_new utilizando el modelo knn.\nImprime las etiquetas predichas para el conjunto de predicciones.\n\n\n# Predict the labs for the X_new\ny_pred = knn.predict(X_new)\n\n# Print the predictions\nprint(\"Predictions: {}\".format(y_pred))\n\nPredictions: [0 1 0]\n\n\nEl modelo ha predicho que los primeros y terceros clientes no se dar√°n de baja en el nuevo array. Pero, ¬øc√≥mo sabemos qu√© tan precisas son estas predicciones? Vamos a explorar c√≥mo medir el rendimiento de un modelo en el pr√≥ximo video.",
    "crumbs": [
      "Aprendizaje supervisado con scikit-learn",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  },
  {
    "objectID": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html#medir-el-rendimiento-del-modelo",
    "href": "01_Aprendizaje_supervisado_con_scikitlearn/01_Clasificacion.html#medir-el-rendimiento-del-modelo",
    "title": "Clasificaci√≥n",
    "section": "Medir el rendimiento del modelo",
    "text": "Medir el rendimiento del modelo\n\nMedir el rendimiento del modelo\n\nEn clasificaci√≥n, se utiliza a menudo la m√©trica de la precisi√≥n.\nPrecisi√≥n: \\[\n\\frac{\\text{Predicciones correctas}}{\\text{total de observaciones}}\n\\]\n¬øC√≥mo medimos la precisi√≥n?\nPodr√≠a calcular la precisi√≥n de los datos utilizados para ajustar el clasificador.\nNO ser√≠a indicativo de la capacidad de generalizaci√≥n\n\nPrecisi√≥n del c√°lculo\n\nEntrenamiento/prueba de la divisi√≥n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_text, y_train, y_test = train_test_split(X, y, text_size=0.3, random_state=21, stratify=y)\n\nknn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(X_train, y_train)\nprint(knn.score(X_text, y_test))\n\n\nComplejidad del modelo\n\nk mayor = modelo menos complejo = puede provocar un ajuste insuficiente (underfitting)\nk menor = modelo m√°s complejo = puede llevar a un sobreajuste (overfitting)\n\n\nComplejidad del modelo y sobreajuste o o ajuste insuficiente\n\ntrain_accuracies = {}\ntest_accuracies = {}\nneighbors = np.arange(1, 26)\nfor neighbor in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=neighbor)\n    knn.fit(X_train, y_train)\n    train_accuracies[neighbor] = knn.score(X_train, y_train)\n    test_accuracies[neighbor] = knn.score(X_test, y_test)\n\nRepresentaci√≥n gr√°fica de nuestros resultados\n\nplt.figure(figsize=(8, 6))\nplt.title(\"KNN: Varying Number of Neighbors\")\nplt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\nplt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\nplt.legend()\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\nCurva de complejidad del modelo\n\n\n\nDivisi√≥n entrenamiento/prueba + c√°lculo de precisi√≥n\n\n\nSobreajuste e infraajuste\n\n\nVisualizar la complejidad del modelo",
    "crumbs": [
      "Aprendizaje supervisado con scikit-learn",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Clasificaci√≥n</span>"
    ]
  }
]